2024-12-18 16:50:24,330 ENVENC TRAINING INFO: Training/evaluation parameters {'per_gpu_train_batch_size': 32, 'per_gpu_eval_batch_size': 128, 'n_gpu': 4, 'num_workers': 4, 'num_train_epochs': 50, 'name': 'Prova', 'save_steps_epochs': 10, 'dataset': 'csv/train_short_frontal_clean.csv', 'gradient_accumulation_steps': 1, 'logging_steps': 100, 'save_steps': 0, 'saved_checkpoints': 'snic2022-5-277/dmolino/EnvEnc_Training/saved_checkpoints', 'logs': 'EnvEnc_Training/logs', 'clip_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Clip_Training/saved_checkpoints/checkpoint_29_epoch_Training_Clip_5e^-5.pt', 'frontal_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Frontal/checkpoint_99_epoch_Training-Frontal-MultiPrompt-New.pt', 'lateral_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Lateral/checkpoint_99_epoch_Training-Lateral-MultiPrompt-New.pt', 'text_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Report_Training/saved_checkpoints/checkpoint_99_epoch_Report_Diffusion_Training-MultiPrompt-New.pt', 'optimus_weights': 'Report_Training/saved_checkpoints/VAE/checkpoint_99_epoch_VAE-Training-Prova1.pt', 'load_envenc': True, 'optimizer': {'params': {'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}, 'type': 'AdamW'}, 'text_emb': False, 'frontal_emb': True, 'lateral_emb': False, 'view': 'lateral', 'device': 'cuda'}
2024-12-18 16:56:31,180 ENVENC TRAINING INFO: Training/evaluation parameters {'per_gpu_train_batch_size': 32, 'per_gpu_eval_batch_size': 128, 'n_gpu': 4, 'num_workers': 4, 'num_train_epochs': 50, 'name': 'Prova', 'save_steps_epochs': 10, 'dataset': 'csv/train_short_frontal_clean.csv', 'gradient_accumulation_steps': 1, 'logging_steps': 100, 'save_steps': 0, 'saved_checkpoints': '../../../snic2022-5-277/dmolino/Report_Training/saved_checkpoints', 'logs': 'EnvEnc_Training/logs', 'clip_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Clip_Training/saved_checkpoints/checkpoint_29_epoch_Training_Clip_5e^-5.pt', 'frontal_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Frontal/checkpoint_99_epoch_Training-Frontal-MultiPrompt-New.pt', 'lateral_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Lateral/checkpoint_99_epoch_Training-Lateral-MultiPrompt-New.pt', 'text_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Report_Training/saved_checkpoints/checkpoint_99_epoch_Report_Diffusion_Training-MultiPrompt-New.pt', 'optimus_weights': 'Report_Training/saved_checkpoints/VAE/checkpoint_99_epoch_VAE-Training-Prova1.pt', 'load_envenc': True, 'optimizer': {'params': {'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}, 'type': 'AdamW'}, 'text_emb': False, 'frontal_emb': True, 'lateral_emb': False, 'view': 'lateral', 'device': 'cuda'}
2024-12-18 16:56:35,885 ENVENC TRAINING INFO: ***** Running training *****
2024-12-18 16:56:35,887 ENVENC TRAINING INFO:   Num examples = 45111
2024-12-18 16:56:35,888 ENVENC TRAINING INFO:   Num Epochs = 50
2024-12-18 16:56:35,888 ENVENC TRAINING INFO:   Number of GPUs = 4
2024-12-18 16:56:35,889 ENVENC TRAINING INFO:   Batch size per GPU = 32
2024-12-18 16:56:35,889 ENVENC TRAINING INFO:   Total train batch size (w. parallel, & accumulation) = 128
2024-12-18 16:56:35,889 ENVENC TRAINING INFO:   Gradient Accumulation steps = 1
2024-12-18 16:56:35,890 ENVENC TRAINING INFO:   Total optimization steps = 70500
2024-12-18 16:56:35,890 ENVENC TRAINING INFO:   warmup steps = 14100
2024-12-18 17:03:07,263 ENVENC TRAINING INFO: Training/evaluation parameters {'per_gpu_train_batch_size': 32, 'per_gpu_eval_batch_size': 128, 'n_gpu': 4, 'num_workers': 4, 'num_train_epochs': 50, 'name': 'Prova', 'save_steps_epochs': 10, 'dataset': 'csv/train_short_frontal_clean.csv', 'gradient_accumulation_steps': 1, 'logging_steps': 100, 'save_steps': 0, 'saved_checkpoints': '../../../snic2022-5-277/dmolino/Report_Training/saved_checkpoints', 'logs': 'EnvEnc_Training/logs', 'clip_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Clip_Training/saved_checkpoints/checkpoint_29_epoch_Training_Clip_5e^-5.pt', 'frontal_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Frontal/checkpoint_99_epoch_Training-Frontal-MultiPrompt-New.pt', 'lateral_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Lateral/checkpoint_99_epoch_Training-Lateral-MultiPrompt-New.pt', 'text_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Report_Training/saved_checkpoints/checkpoint_99_epoch_Report_Diffusion_Training-MultiPrompt-New.pt', 'optimus_weights': 'Report_Training/saved_checkpoints/VAE/checkpoint_99_epoch_VAE-Training-Prova1.pt', 'load_envenc': True, 'optimizer': {'params': {'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}, 'type': 'AdamW'}, 'text_emb': False, 'frontal_emb': True, 'lateral_emb': False, 'view': 'lateral', 'device': 'cuda'}
2024-12-18 17:03:13,650 ENVENC TRAINING INFO: ***** Running training *****
2024-12-18 17:03:13,652 ENVENC TRAINING INFO:   Num examples = 45111
2024-12-18 17:03:13,653 ENVENC TRAINING INFO:   Num Epochs = 50
2024-12-18 17:03:13,653 ENVENC TRAINING INFO:   Number of GPUs = 4
2024-12-18 17:03:13,653 ENVENC TRAINING INFO:   Batch size per GPU = 32
2024-12-18 17:03:13,654 ENVENC TRAINING INFO:   Total train batch size (w. parallel, & accumulation) = 128
2024-12-18 17:03:13,654 ENVENC TRAINING INFO:   Gradient Accumulation steps = 1
2024-12-18 17:03:13,654 ENVENC TRAINING INFO:   Total optimization steps = 70500
2024-12-18 17:03:13,655 ENVENC TRAINING INFO:   warmup steps = 14100
2024-12-18 17:08:03,357 ENVENC TRAINING INFO: Training/evaluation parameters {'per_gpu_train_batch_size': 32, 'per_gpu_eval_batch_size': 128, 'n_gpu': 4, 'num_workers': 4, 'num_train_epochs': 50, 'name': 'Prova', 'save_steps_epochs': 10, 'dataset': 'csv/train_short_frontal_clean.csv', 'gradient_accumulation_steps': 1, 'logging_steps': 100, 'save_steps': 0, 'saved_checkpoints': '../../../snic2022-5-277/dmolino/Report_Training/saved_checkpoints', 'logs': 'EnvEnc_Training/logs', 'clip_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Clip_Training/saved_checkpoints/checkpoint_29_epoch_Training_Clip_5e^-5.pt', 'frontal_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Frontal/checkpoint_99_epoch_Training-Frontal-MultiPrompt-New.pt', 'lateral_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/CXR_Training/saved_checkpoints/Lateral/checkpoint_99_epoch_Training-Lateral-MultiPrompt-New.pt', 'text_weights': '/mimer/NOBACKUP/groups/snic2022-5-277/dmolino/Report_Training/saved_checkpoints/checkpoint_99_epoch_Report_Diffusion_Training-MultiPrompt-New.pt', 'optimus_weights': 'Report_Training/saved_checkpoints/VAE/checkpoint_99_epoch_VAE-Training-Prova1.pt', 'load_envenc': True, 'optimizer': {'params': {'eps': 1e-08, 'lr': 5e-05, 'weight_decay': 0.0001}, 'type': 'AdamW'}, 'text_emb': False, 'frontal_emb': True, 'lateral_emb': False, 'view': 'lateral', 'device': 'cuda'}
2024-12-18 17:08:09,369 ENVENC TRAINING INFO: ***** Running training *****
2024-12-18 17:08:09,371 ENVENC TRAINING INFO:   Num examples = 45111
2024-12-18 17:08:09,373 ENVENC TRAINING INFO:   Num Epochs = 50
2024-12-18 17:08:09,374 ENVENC TRAINING INFO:   Number of GPUs = 4
2024-12-18 17:08:09,374 ENVENC TRAINING INFO:   Batch size per GPU = 32
2024-12-18 17:08:09,374 ENVENC TRAINING INFO:   Total train batch size (w. parallel, & accumulation) = 128
2024-12-18 17:08:09,375 ENVENC TRAINING INFO:   Gradient Accumulation steps = 1
2024-12-18 17:08:09,375 ENVENC TRAINING INFO:   Total optimization steps = 70500
2024-12-18 17:08:09,375 ENVENC TRAINING INFO:   warmup steps = 14100
